{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7887c9c06368cb33",
   "metadata": {},
   "source": [
    "# Geolife Checks\n",
    "\n",
    "In this notebook, we will import and initialise all models to ensure that they are working correctly on the **Geolife Dataset**!\n",
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T02:47:33.265831Z",
     "start_time": "2024-05-21T02:47:32.119578Z"
    }
   },
   "outputs": [],
   "source": [
    "GPU = 0\n",
    "%cd ..\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# Print Python Version & PyTorch version\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "print(f\"Python version\\t=\\t{sys.version}\\nPyTorch version\\t=\\t{torch.__version__}\")\n",
    "# Make torch deterministic\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece647d912d1da2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T02:47:33.282152Z",
     "start_time": "2024-05-21T02:47:33.267128Z"
    }
   },
   "outputs": [],
   "source": [
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "if RunningInCOLAB:\n",
    "    # Move to default colab folder\n",
    "    %cd /content\n",
    "    # Check if repository is already cloned\n",
    "    if not os.path.isdir(\"stg\"):\n",
    "        # Clone repository\n",
    "        !git clone {config.GITHUB_URL} {config.MODULE_NAME}\n",
    "    # Change to repository directory\n",
    "    %cd {config.MODULE_NAME}\n",
    "    # Only install requirements not already installed by Colab\n",
    "    # !pip install opacus\n",
    "    # SLOW: Only execute the following line if you encounter an error regarding a package not being installed\n",
    "    # !pip install -r requirements.txt\n",
    "else:\n",
    "    import sys\n",
    "    # Add parent directory (absolute!) to path\n",
    "    sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc968159ffa1116",
   "metadata": {},
   "source": [
    "## Geolife Dataset\n",
    "\n",
    "We use the Geolife dataset as a sample dataset to test the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e6beb721e7f728",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T02:47:37.412194Z",
     "start_time": "2024-05-21T02:47:33.282937Z"
    }
   },
   "outputs": [],
   "source": [
    "from conv_gan.datasets import get_dataset, Datasets, ZeroPadding\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "ds = get_dataset(Datasets.GEOLIFE, return_labels=True)\n",
    "# Print Shape of one sample\n",
    "print(f\"Sample:\\t({len(ds[0][0])}, {ds[0][0][0].shape}\")\n",
    "print(f\"Number of Trajecotries:\\t{len(ds)}\")\n",
    "   \n",
    "from torch.utils.data import DataLoader\n",
    "padding_fn = ZeroPadding(\n",
    "    return_len=True,\n",
    "    return_labels=True,\n",
    "    feature_first=True,\n",
    ")\n",
    "dl = DataLoader(ds, batch_size=BATCH_SIZE, collate_fn=padding_fn)\n",
    "# Print Shape of one batch\n",
    "batch, lengths, labels = next(iter(dl))\n",
    "print(f\"Batch:\\t{len(batch)}\")\n",
    "print(f\"Labels:\\t{len(labels)}\")\n",
    "for i in range(len(batch)):\n",
    "    print(f\"Batch Feature #{i}:\\t{batch[i].shape}\")\n",
    "print(f\"Lengths:\\t\\t\\t{len(lengths)}: {lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf13b02ce7f545c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T02:48:29.359565Z",
     "start_time": "2024-05-21T02:47:37.413155Z"
    }
   },
   "outputs": [],
   "source": [
    "from conv_gan.utils.visualise import plot_pointclouds\n",
    "\n",
    "# Print Point Cloud\n",
    "TRAJ_NUM = 1000\n",
    "original_gl_samples = [ds[i][0][0] for i in np.random.randint(0, len(ds), TRAJ_NUM)]\n",
    "points = torch.cat(original_gl_samples, dim=0).view(-1, 2).cpu().numpy()\n",
    "print(points.shape)\n",
    "_ = plot_pointclouds(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95715805df4c5154",
   "metadata": {},
   "source": [
    "## Noise-TrajGAN: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835cfaf6b350e359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T02:48:29.398265Z",
     "start_time": "2024-05-21T02:48:29.361184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "FEATURES = ds.features\n",
    "LATENT_DIM = 256\n",
    "NOISE_DIM = 28\n",
    "# Training Parameters\n",
    "# Choose epochs such that we have around 10k steps total\n",
    "EPOCHS = 10000 // len(dl) + 1\n",
    "print(f\"Epochs:\\t{EPOCHS}\\nSteps:\\t{EPOCHS * len(dl)}\")\n",
    "\n",
    "WGAN = True\n",
    "LP = True  # Lipschitz Penalty required!\n",
    "LR_G = 0.0001\n",
    "LR_D = 0.0001\n",
    "N_CRITIC = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f872a032d5dd3103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T02:48:30.179467Z",
     "start_time": "2024-05-21T02:48:29.399396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Noise-TrajGAN\n",
    "from conv_gan.models.noise_trajgan import Noise_TrajGAN\n",
    "\n",
    "name = f'NTG_GL_G{LR_G}_{N_CRITIC}xD{LR_D}_L{LATENT_DIM}_N{NOISE_DIM}_B{BATCH_SIZE}_{\"WGAN\" if WGAN else \"GAN\"}'\n",
    "\n",
    "# Create a Noise-TrajGAN model\n",
    "ntg = Noise_TrajGAN(\n",
    "    features=FEATURES,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    noise_dim=NOISE_DIM,\n",
    "    wgan=WGAN,\n",
    "    gradient_penalty=LP,\n",
    "    lipschitz_penalty=LP,\n",
    "    lr_g=LR_G,\n",
    "    lr_d=LR_D,\n",
    "    gpu = 0,\n",
    "    name=name,\n",
    ")\n",
    "# Print and compare feature number of generator and discriminator\n",
    "count_params_torch = lambda model: sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Generator Parameters:\\t\\t{count_params_torch(ntg.gen)}\")\n",
    "print(f\"Discriminator Parameters:\\t{count_params_torch(ntg.dis)}\")\n",
    "print(f\"Relationship [G / D]:\\t\\t{count_params_torch(ntg.gen) / count_params_torch(ntg.dis) * 100 :.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e48e7b0c7243e51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T02:48:30.465538Z",
     "start_time": "2024-05-21T02:48:30.180667Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print an initial output of NTG\n",
    "fake = ntg.generate(1000, 28)\n",
    "# print Results\n",
    "for i, feature in enumerate(fake):\n",
    "    print(f\"Feature {i}:\\t{len(feature), len(feature[0]), len(feature[0][0])}\")\n",
    "latlon = np.array(fake[0])\n",
    "# Reshape to (-1, 2)\n",
    "points = latlon.reshape(-1, 2)\n",
    "print(points.shape)\n",
    "plot_pointclouds(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef3865c8e2a6646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "ntg.training_loop(dl, epochs=EPOCHS, dataset_name=Datasets.GEOLIFE, n_critic=N_CRITIC, plot_freq=200, save_freq=-1, tensorboard=True, notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4d6c92432926d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T03:20:33.824026Z",
     "start_time": "2024-05-21T03:20:31.066273Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print an initial output of NTG\n",
    "fake = ntg.generate(1000, 28)\n",
    "# print Results\n",
    "for i, feature in enumerate(fake):\n",
    "    print(f\"Feature {i}:\\t{len(feature), len(feature[0]), len(feature[0][0])}\")\n",
    "latlon = np.array(fake[0])\n",
    "# Reshape to (-1, 2)\n",
    "points = latlon.reshape(-1, 2)\n",
    "print(points.shape)\n",
    "plot_pointclouds(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d2f36d9efb17a9",
   "metadata": {},
   "source": [
    "## Noise-TrajGAN with Differential Privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabcdae2430198e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T03:20:35.452691Z",
     "start_time": "2024-05-21T03:20:35.401153Z"
    }
   },
   "outputs": [],
   "source": [
    "# DP Parameters\n",
    "EPSILON = 10.0\n",
    "ACCOUNTANT = 'prv'  # Default is 'prv', but I found that 'rdp' is more stable in some situation \n",
    "MAX_GRAD_NORM = 0.1\n",
    "# Delta should be 1/n where n is the number of samples according to DPfy-ML\n",
    "DELTA = 1 / len(ds)\n",
    "print(f\"Epsilon:\\t{EPSILON:.1f}\\nDelta:\\t\\t{DELTA:.2e}\\nMax Grad Norm:\\t{MAX_GRAD_NORM}\\nAccountant:\\t{ACCOUNTANT}\")\n",
    "\n",
    "\n",
    "DP_IN_DIS = False  # Whether to apply DP-SGD to discriminator or generator\n",
    "# WGAN Clipping does not work if DP is applied to the discriminator\n",
    "LP = not DP_IN_DIS\n",
    "WGAN = True\n",
    "if not DP_IN_DIS and N_CRITIC > 1:\n",
    "    print(\"Warning: Training with DP and N_CRITIC is a bit of a gamble because we might actually be wasting privacy budget on the discriminator which does not even uses DP.\")\n",
    "\n",
    "# Increase learning rate of DP model to make up for the gradient clipping\n",
    "if DP_IN_DIS:\n",
    "    LR_D = LR_D / MAX_GRAD_NORM \n",
    "else:\n",
    "    LR_G = LR_G / MAX_GRAD_NORM\n",
    "\n",
    "print(f\"LR_G:\\t{LR_G}\\nLR_D:\\t{LR_D}\")\n",
    "\n",
    "# Create new DataLoader\n",
    "# The number of steps should be the same as without DP, but DP-SGD works better for large batches\n",
    "# --> Increase batches and epochs by same factor\n",
    "FACTOR = 10\n",
    "DP_BATCH_SIZE = BATCH_SIZE * FACTOR\n",
    "DP_EPOCHS = EPOCHS * FACTOR\n",
    "dp_dl = DataLoader(ds, batch_size=DP_BATCH_SIZE, collate_fn=padding_fn)\n",
    "print(f\"Batch Size:\\t{DP_BATCH_SIZE}\\nEpochs:\\t\\t{DP_EPOCHS}\\nSteps:\\t\\t{DP_EPOCHS * len(dp_dl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb251205d0575dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T03:20:36.252163Z",
     "start_time": "2024-05-21T03:20:36.196327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize DP-Noise-TrajGAN\n",
    "name = f'DP-NTG_GL_G{LR_G}_{N_CRITIC}xD{LR_D}_L{LATENT_DIM}_N{NOISE_DIM}_B{DP_BATCH_SIZE}_C{MAX_GRAD_NORM}'\n",
    "\n",
    "dp_ntg = Noise_TrajGAN(\n",
    "    features=FEATURES,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    noise_dim=NOISE_DIM,\n",
    "    lr_g=LR_G,\n",
    "    lr_d=LR_D,\n",
    "    gpu = 0,\n",
    "    name=name,\n",
    "    wgan=WGAN,\n",
    "    gradient_penalty=LP,\n",
    "    lipschitz_penalty=LP,\n",
    "    dp=True,\n",
    "    dp_in_dis=DP_IN_DIS,\n",
    "    privacy_accountant=ACCOUNTANT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5b3068358dedae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T03:20:42.756621Z",
     "start_time": "2024-05-21T03:20:36.985397Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize DP --> Returns DP dataloader\n",
    "dp_dl = dp_ntg.init_dp(\n",
    "    dataloader=dp_dl,\n",
    "    epochs=DP_EPOCHS,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    target_epsilon=EPSILON,\n",
    "    delta=DELTA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c14af890303b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T03:20:43.006407Z",
     "start_time": "2024-05-21T03:20:42.775747Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print an initial output of NTG\n",
    "fake = ntg.generate(1000, 28)\n",
    "# print Results\n",
    "for i, feature in enumerate(fake):\n",
    "    print(f\"Feature {i}:\\t{len(feature), len(feature[0]), len(feature[0][0])}\")\n",
    "latlon = np.array(fake[0])\n",
    "# Reshape to (-1, 2)\n",
    "points = latlon.reshape(-1, 2)\n",
    "print(points.shape)\n",
    "plot_pointclouds(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74470ed99a03aaeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T03:21:52.286120Z",
     "start_time": "2024-05-21T03:20:43.007460Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the DP Model\n",
    "dp_ntg.training_loop(dp_dl, epochs=DP_EPOCHS, dataset_name=Datasets.GEOLIFE, n_critic=N_CRITIC, plot_freq=200, save_freq=-1, tensorboard=True, notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf20350af93a8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T03:22:04.716480Z",
     "start_time": "2024-05-21T03:22:04.500150Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print an initial output of NTG\n",
    "fake = ntg.generate(1000, 28)\n",
    "# print Results\n",
    "for i, feature in enumerate(fake):\n",
    "    print(f\"Feature {i}:\\t{len(feature), len(feature[0]), len(feature[0][0])}\")\n",
    "latlon = np.array(fake[0])\n",
    "# Reshape to (-1, 2)\n",
    "points = latlon.reshape(-1, 2)\n",
    "print(points.shape)\n",
    "plot_pointclouds(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a947f79e9782f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T03:22:05.248758Z",
     "start_time": "2024-05-21T03:22:04.907454Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print an initial output of NTG\n",
    "fake = ntg.generate(1000, 28)\n",
    "# print Results\n",
    "for i, feature in enumerate(fake):\n",
    "    print(f\"Feature {i}:\\t{len(feature), len(feature[0]), len(feature[0][0])}\")\n",
    "latlon = np.array(fake[0])\n",
    "# Reshape to (-1, 2)\n",
    "points = latlon.reshape(-1, 2)\n",
    "print(points.shape)\n",
    "plot_pointclouds(points)# Print resulting privacy loss\n",
    "print(\"Final Delta:\\t\", dp_ntg.delta)\n",
    "print(\"Final Epsilon:\\t\", dp_ntg.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b0ae683d9e4b35",
   "metadata": {},
   "source": [
    "## Baseline CNN-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243a74afa6fe9e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T03:22:08.741548Z",
     "start_time": "2024-05-21T03:22:06.143167Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import CNN-GAN\n",
    "import config\n",
    "from conv_gan.models.dcgan import DCGan\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# It is really simple to run the model but it could be made even more simple if required... \n",
    "# For more info on these hyperparameters look at 5-fold.py in the root directory.\n",
    "opt = {\n",
    "        \"file\":'geolife', # This specifically is used for saving files - but also for \n",
    "        \"epochs\":200, \n",
    "        \"batch_size\":64, \n",
    "        \"lr\":0.0002, \n",
    "        \"g_factor\":1.0, \n",
    "        \"b1\":0.5, \n",
    "        \"evaluate\":False, \n",
    "        \"b2\":0.999, \n",
    "        \"n_cpu\":8, \n",
    "        \"latent_dim\":100, \n",
    "        \"img_size\":24, \n",
    "        \"channels\":1, \n",
    "        \"sample_interval\":1000, \n",
    "        \"load_params\":0, \n",
    "        \"g\":0, \n",
    "        \"schedule\":None, \n",
    "        \"plot_points\":False\n",
    "    }\n",
    "if opt['file'] == \"geolife\":\n",
    "    data =  pd.read_csv(config.BASE_DIR + \"data/geolife/restricted_geolife.csv\")\n",
    "else:   \n",
    "    data = pd.read_csv(config.BASE_DIR + \"data/fs_nyc/restricted_foursquare.csv\")\n",
    "\n",
    "# Get all the trajectories in the format I want (maximum length is 144)\n",
    "# In theory the pre-processing should prevent larger sized trajectories, but we do this just incase.\n",
    "trajectories = [traj.values.tolist()[:144] for tid, traj in data.groupby('tid') ]\n",
    "\n",
    "# Split data into 2/3 (train) and 1/3 (test)\n",
    "kf = KFold(n_splits=3)\n",
    "train, test = list(kf.split(trajectories))[0]\n",
    "\n",
    "dcgan = DCGan(opt, mainData=[trajectories[i] for i in train], testData=[trajectories[i] for i in test],fold=0)\n",
    "# dcgan.training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98244956c08dbae0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T03:22:08.742994Z",
     "start_time": "2024-05-21T03:22:08.742752Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ecb1d24c6e6ea1f",
   "metadata": {},
   "source": [
    "## DP CNN-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470c081abe2bf02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T03:22:08.744001Z",
     "start_time": "2024-05-21T03:22:08.743738Z"
    }
   },
   "outputs": [],
   "source": [
    "LR_D = opt[\"lr\"]\n",
    "LR_G = opt[\"lr\"]\n",
    "\n",
    "# DP Parameters\n",
    "EPSILON = 10.0\n",
    "# Delta should be 1/n where n is the number of samples according to DPfy-ML\n",
    "DELTA = 1 / len(ds)\n",
    "print(f\"Epsilon:\\t{EPSILON:.1f}\\nDelta:\\t\\t{DELTA:.2e}\")\n",
    "ACCOUNTANT = 'prv'  # Default is 'prv', but I found that 'rdp' is more stable\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "DP_IN_DIS = False  # Whether to apply DP-SGD to discriminator or generator\n",
    "# Gradient Clipping does not work if DP is applied to the discriminator\n",
    "LP = not DP_IN_DIS\n",
    "WGAN = True\n",
    "\n",
    "# Increase learning rate of DP model to make up for the gradient clipping\n",
    "if DP_IN_DIS:\n",
    "    LR_D = LR_D / MAX_GRAD_NORM \n",
    "else:\n",
    "    LR_G = LR_G / MAX_GRAD_NORM\n",
    "\n",
    "print(f\"LR_G:\\t\\t{LR_G}\\nLR_D:\\t\\t{LR_D}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b39aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T03:22:11.949936Z",
     "start_time": "2024-05-21T03:22:09.694170Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import CNN-GAN\n",
    "import config\n",
    "from conv_gan.models.dcgan import DCGan\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# It is really simple to run the model but it could be made even more simple if required... \n",
    "# For more info on these hyperparameters look at 5-fold.py in the root directory.\n",
    "opt = {\n",
    "        \"file\":'geolife', # This specifically is used for saving files - but also for \n",
    "        \"n_epochs\":200, \n",
    "        \"batch_size\":64, \n",
    "        \"lr\":0.0002, \n",
    "        \"g_factor\":1.0, \n",
    "        \"b1\":0.5, \n",
    "        \"evaluate\":False, \n",
    "        \"b2\":0.999, \n",
    "        \"n_cpu\":8, \n",
    "        \"latent_dim\":100, \n",
    "        \"img_size\":24, \n",
    "        \"channels\":1, \n",
    "        \"sample_interval\":1000, \n",
    "        \"load_params\":0, \n",
    "        \"g\":0, \n",
    "        \"schedule\":None, \n",
    "        \"plot_points\":False\n",
    "    }\n",
    "if opt['file'] == \"geolife\":\n",
    "    data =  pd.read_csv(config.BASE_DIR + \"data/geolife/restricted_geolife.csv\")\n",
    "else:   \n",
    "    data = pd.read_csv(config.BASE_DIR + \"data/fs_nyc/restricted_foursquare.csv\")\n",
    "\n",
    "# Get all the trajectories in the format I want (maximum length is 144)\n",
    "# In theory the pre-processing should prevent larger sized trajectories, but we do this just incase.\n",
    "trajectories = [traj.values.tolist()[:144] for tid, traj in data.groupby('tid') ]\n",
    "\n",
    "# Split data into 2/3 (train) and 1/3 (test)\n",
    "kf = KFold(n_splits=3)\n",
    "train, test = list(kf.split(trajectories))[0]\n",
    "\n",
    "dcgan = DCGan(\n",
    "    opt,\n",
    "    mainData=[trajectories[i] for i in train],\n",
    "    testData=[trajectories[i] for i in test],\n",
    "    fold=0,\n",
    "    dp=True,\n",
    "    gpu=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa5dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db98b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "convGAN",
   "language": "python",
   "name": "convgan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
